{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Data visualization library\nimport seaborn as sns # Data visualization library\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T02:14:07.133603Z","iopub.execute_input":"2022-06-20T02:14:07.134353Z","iopub.status.idle":"2022-06-20T02:14:07.148487Z","shell.execute_reply.started":"2022-06-20T02:14:07.134302Z","shell.execute_reply":"2022-06-20T02:14:07.147589Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Seasonal Decompositionm \nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Lag Scatter plots\nfrom pandas.plotting import lag_plot\n\n# Autocorrelation plots\nfrom pandas.plotting import autocorrelation_plot\n\n# Numerical Computation and Model Metrics \nfrom math import sqrt\nfrom numpy import mean\nfrom sklearn.metrics import mean_squared_error\n\n# Statistical Tests and plots for checking white noise \nfrom random import gauss\nfrom random import seed\nfrom pandas import Series\nfrom pandas.plotting import autocorrelation_plot ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:50:42.108378Z","iopub.execute_input":"2022-06-20T03:50:42.109627Z","iopub.status.idle":"2022-06-20T03:50:42.115137Z","shell.execute_reply.started":"2022-06-20T03:50:42.109535Z","shell.execute_reply":"2022-06-20T03:50:42.114447Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"# Questions to be asked:\n1. Ask relevant questions related to the timestamps\n2. Whose timestamp? How are those timestamps generated?\n3. At which instant were those timestamps recorded?\n4. Guesstimating time stamps to make sense of data\n5. The questions determine the data pipeline \n6. Local or universal time. Most timestamps are stored according to the UTC\n7. Guesstimating timestamps to make sense of data\n8. Is it user behaviour or network behaviour?\n9. Date-specific API calls???\n10. Psychological Time-Discounting is a manifestation of a phenomenon known as psychological distance, which names our tendency to be more optimistic(and less realistic) when making assessments or estimates that are more distant from us. \n31. Humans know when time is passing.","metadata":{}},{"cell_type":"code","source":"# Treat the first column as index\nseries = pd.read_csv(\"../input/daily-total-female-births-in-california-1959/daily-total-female-births-CA.csv\", header=0, index_col=0, parse_dates=True, squeeze=True)\nseries.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.153375Z","iopub.execute_input":"2022-06-20T02:14:07.153722Z","iopub.status.idle":"2022-06-20T02:14:07.168073Z","shell.execute_reply.started":"2022-06-20T02:14:07.153685Z","shell.execute_reply":"2022-06-20T02:14:07.167066Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Do the usual descriptive analysis stuff\nprint(series.size)\nprint(series['1959-01'])\nprint(series.describe())","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.176607Z","iopub.execute_input":"2022-06-20T02:14:07.177213Z","iopub.status.idle":"2022-06-20T02:14:07.189411Z","shell.execute_reply.started":"2022-06-20T02:14:07.177175Z","shell.execute_reply":"2022-06-20T02:14:07.188423Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Basic Feature Engineering\nTime Series data must be re-framed as a supervised learning dataset before we can start using\nmachine learning algorithms. There is no concept of input and output features in time series.\nInstead, we must choose the variable to be predicted and use feature engineering to construct\nall of the inputs that will be used to make predictions for future time steps. In this tutorial,\nyou will discover how to perform feature engineering on time series data with Python to model\nyour time series problem with machine learning algorithms.\nAfter completing this tutorial, you will know:\n*  The rationale and goals of feature engineering time series data.\n*  How to develop basic date-time based input features.\n* How to develop more sophisticated lag and sliding window summary statistics features.\nLet’s dive in\n\nI may enumerate with all the problems of time-stamp and consider what might be useful for the problem, such as:\n* Minutes Elapsed\n* Hour of Day\n* Business Hours\n\nLag features:\n* Weekend or not\n* Season of the year\n* Business quarter of the year\n* Daylight savings or not\n* Public holiday or not\n* Leap year or not\n\nAdding domain-specific features is a good start for time-series.","metadata":{}},{"cell_type":"markdown","source":"**Creating Rolling and Expanding Window summary statistic features**","metadata":{}},{"cell_type":"code","source":"# Create Rolling Window Statistics\ntemps = pd.DataFrame(series.values)\nwidth = 3\nshifted = temps.shift(width - 1)\nwindow = shifted.rolling(window=width)\ndataframe = pd.concat([window.min(), window.mean(), window.max(), temps], axis=1)\ndataframe.columns = ['min', 'mean', 'max', 't+1']\nprint(dataframe.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.307622Z","iopub.execute_input":"2022-06-20T02:14:07.308159Z","iopub.status.idle":"2022-06-20T02:14:07.322217Z","shell.execute_reply.started":"2022-06-20T02:14:07.308106Z","shell.execute_reply":"2022-06-20T02:14:07.321166Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(temps)\nprint()\nprint(shifted)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.324238Z","iopub.execute_input":"2022-06-20T02:14:07.324826Z","iopub.status.idle":"2022-06-20T02:14:07.334318Z","shell.execute_reply.started":"2022-06-20T02:14:07.324784Z","shell.execute_reply":"2022-06-20T02:14:07.333454Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Create Expanding Window Statistics\ntemps = pd.DataFrame(series.values)\nwindow = temps.expanding()\ndataframe = pd.concat([window.min(), window.mean(), window.max(), temps.shift(-1)], axis=1)\ndataframe.columns = ['min', 'mean', 'max', 't+1']\nprint(dataframe.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.335475Z","iopub.execute_input":"2022-06-20T02:14:07.336115Z","iopub.status.idle":"2022-06-20T02:14:07.349167Z","shell.execute_reply.started":"2022-06-20T02:14:07.336073Z","shell.execute_reply":"2022-06-20T02:14:07.348210Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series Visualization","metadata":{}},{"cell_type":"markdown","source":"1. Line Plots\n2. Histograms and Density Plots\n3. Box and Whisker Plots\n4. Heat Maps\n5. Lag Plots or Scatter Plots\n6. Autocorrelation Plots","metadata":{}},{"cell_type":"code","source":"# Reading data in a proper format\nmelbourne = pd.read_csv(\"../input/melbourne-temperature/daily-minimum-temperatures-in-me.csv\", header=0, index_col=0, parse_dates=True, squeeze=True, on_bad_lines='warn')\nmelbourne = melbourne.apply(lambda x:x.replace('?', ''))\nmelbourne = melbourne.astype(float)\n\n# Plotting the series\nmelbourne.plot(style='--') #'k--' is one of the type \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.351498Z","iopub.execute_input":"2022-06-20T02:14:07.352474Z","iopub.status.idle":"2022-06-20T02:14:07.617905Z","shell.execute_reply.started":"2022-06-20T02:14:07.352426Z","shell.execute_reply":"2022-06-20T02:14:07.617133Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Create subplots as well\ngroups = melbourne.groupby(pd.Grouper(freq='A'))\nyears = pd.DataFrame()\nfor name, group in groups:\n    years[name.year] = group.values\n    \nyears.plot(subplots=True, legend=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:07.619236Z","iopub.execute_input":"2022-06-20T02:14:07.619843Z","iopub.status.idle":"2022-06-20T02:14:08.595284Z","shell.execute_reply.started":"2022-06-20T02:14:07.619801Z","shell.execute_reply":"2022-06-20T02:14:08.594445Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#Histogram and density plots\nmelbourne.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:08.596579Z","iopub.execute_input":"2022-06-20T02:14:08.596823Z","iopub.status.idle":"2022-06-20T02:14:08.933630Z","shell.execute_reply.started":"2022-06-20T02:14:08.596793Z","shell.execute_reply":"2022-06-20T02:14:08.932798Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Draw a KDE to understand the data better\nmelbourne.plot(kind='kde')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:08.934769Z","iopub.execute_input":"2022-06-20T02:14:08.934988Z","iopub.status.idle":"2022-06-20T02:14:09.155464Z","shell.execute_reply.started":"2022-06-20T02:14:08.934962Z","shell.execute_reply":"2022-06-20T02:14:09.154646Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"#Box and whisker plots by interval\nyears.boxplot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.156760Z","iopub.execute_input":"2022-06-20T02:14:09.156981Z","iopub.status.idle":"2022-06-20T02:14:09.406891Z","shell.execute_reply.started":"2022-06-20T02:14:09.156953Z","shell.execute_reply":"2022-06-20T02:14:09.406111Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"len(groups)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.408921Z","iopub.execute_input":"2022-06-20T02:14:09.409127Z","iopub.status.idle":"2022-06-20T02:14:09.414484Z","shell.execute_reply.started":"2022-06-20T02:14:09.409103Z","shell.execute_reply":"2022-06-20T02:14:09.413637Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Creating a boxplot of monthly data. \none_year = melbourne['1990']\ngroups = one_year.groupby(pd.Grouper(freq='M'))\n\n# The entire group will be grouped by months, and every succeeding month is stacked in a next column\nmonths = pd.concat([pd.DataFrame(x[1].values) for x in groups], axis=1)\nmonths = pd.DataFrame(months)\nmonths.columns = range(1, 13)\nmonths.boxplot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.416156Z","iopub.execute_input":"2022-06-20T02:14:09.416445Z","iopub.status.idle":"2022-06-20T02:14:09.692294Z","shell.execute_reply.started":"2022-06-20T02:14:09.416406Z","shell.execute_reply":"2022-06-20T02:14:09.691475Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"years","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.693908Z","iopub.execute_input":"2022-06-20T02:14:09.694193Z","iopub.status.idle":"2022-06-20T02:14:09.719191Z","shell.execute_reply.started":"2022-06-20T02:14:09.694154Z","shell.execute_reply":"2022-06-20T02:14:09.718630Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Heatmaps. pd.Grouper. If group is 'A', then data for each of the 12 months will be shown\ngroups = melbourne.groupby(pd.Grouper(freq='A'))\nyears = pd.DataFrame()\nfor name, group in groups:\n    years[name.year]=group.values\nyears = years.T\nyears = pd.DataFrame(years)\nplt.matshow(years, interpolation=None, aspect='auto')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.720076Z","iopub.execute_input":"2022-06-20T02:14:09.720505Z","iopub.status.idle":"2022-06-20T02:14:09.951257Z","shell.execute_reply.started":"2022-06-20T02:14:09.720474Z","shell.execute_reply":"2022-06-20T02:14:09.948497Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"years","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.952515Z","iopub.execute_input":"2022-06-20T02:14:09.953342Z","iopub.status.idle":"2022-06-20T02:14:09.989093Z","shell.execute_reply.started":"2022-06-20T02:14:09.953297Z","shell.execute_reply":"2022-06-20T02:14:09.988403Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Do the same for a particular month as well","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.990530Z","iopub.execute_input":"2022-06-20T02:14:09.991093Z","iopub.status.idle":"2022-06-20T02:14:09.995795Z","shell.execute_reply.started":"2022-06-20T02:14:09.991051Z","shell.execute_reply":"2022-06-20T02:14:09.995098Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"lag_plot(melbourne)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:09.997805Z","iopub.execute_input":"2022-06-20T02:14:09.998031Z","iopub.status.idle":"2022-06-20T02:14:10.128615Z","shell.execute_reply.started":"2022-06-20T02:14:09.998004Z","shell.execute_reply":"2022-06-20T02:14:10.127960Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Create multiple scatter plots for different lags\nvalues = pd.DataFrame(melbourne.values)\nlags = 7\ncolumns = [values]\n\n# Creating columns for 't-1', 't-2', 't-3',...., 't-n'th lags \nfor i in range(1, (lags+1)):\n    columns.append(values.shift(i))\ndataframe = pd.concat(columns, axis=1)\n\ncolumns = ['t']\n\n# Appending name to existing column names\nfor i in range(1, (lags+1)):\n    columns.append('t-' + str(i))\ndataframe.columns = columns\nplt.figure(i)\n\n# Plotting the data\nfor i in range(1, (lags+1)):\n    plt.scatter(x=dataframe['t'].values, y=dataframe['t-'+str(i)].values)\n    \n    # Okay, I can plot the subplots in this manner\n    ax = plt.subplot(240+i)\n    \n    # Setting the title\n    ax.set_title('t vs. t-' + str(i))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:10.130078Z","iopub.execute_input":"2022-06-20T02:14:10.130320Z","iopub.status.idle":"2022-06-20T02:14:10.822406Z","shell.execute_reply.started":"2022-06-20T02:14:10.130288Z","shell.execute_reply":"2022-06-20T02:14:10.821582Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(melbourne)\nplt.show()\n\n# Observations\n# The given data has a very strong seasonal component","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:10.823951Z","iopub.execute_input":"2022-06-20T02:14:10.824417Z","iopub.status.idle":"2022-06-20T02:14:11.006306Z","shell.execute_reply.started":"2022-06-20T02:14:10.824373Z","shell.execute_reply":"2022-06-20T02:14:11.005631Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Resampling and Interpolation\n1. **Upsampling** - For eg., from hours to minutes. Using interpolation - Depends upon the use-case\n2. **Downsampling** - For eg., from minutes to hours. using aggregation(mean, median and mode) - Depends upon the use-case","metadata":{}},{"cell_type":"markdown","source":"1. **Upsampling calls for interpolations, which can be linear as well as complex**\n2. **Situations under which one would do downsampling of data**\n    *  When we want to study observations from a particular time frame\n    *  The original resolution of the data isn't sensible\n    *  Match against data at a lower frequency. In such cases one would simply aggregate the data or downsample rather than simply aggregating points\n3. **Situations under which one would do upsampling of data**\n    *  Irregular time series\n    *  Inputs are sampled at different frequencies\n    *  Knowledge of time-series dynamics determines the interpolation\n4. **Why I would even need to upsample data**\n    *  Upsampling data - here, resampling data, if data isn't available at the same frequency at which one would want to make predictions\n    *  Feature Engineering - create more features, helps in deriving more useful insights \n5. **Identify the problem statement. See what is being asked to predict. Then check if the data is originally sampled at the asked frequency. If yes, then go ahead. \n     Else resampling would be required**","metadata":{}},{"cell_type":"code","source":"# Let's try and test upsampling on shampoo dataset\ndef parser(x):\n    return pd.datetime.strptime('190'+x, '%Y-%m')\n\nseries = pd.read_csv('../input/shampoo-saled-dataset/shampoo_sales.csv', header=0, index_col=0, squeeze=True, parse_dates=True, date_parser=parser)\n\n# The series is automatically converted to '%Y-%M-%d' type format \nprint(series)\nprint(\"\\n\\n\")\nplt.plot(series)\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.007657Z","iopub.execute_input":"2022-06-20T02:14:11.008132Z","iopub.status.idle":"2022-06-20T02:14:11.188056Z","shell.execute_reply.started":"2022-06-20T02:14:11.008089Z","shell.execute_reply":"2022-06-20T02:14:11.187158Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Upsampling monthly data to daily data \nupsampled = series.resample('D').mean()\n\n# Interpolate 'NaN' values with linear interpolation \ninterpolated = upsampled.interpolate(method='spline', order=2)\n\n# Print the values\nprint(interpolated.head(32))\nprint(\"\\n\\n\")\nplt.plot(interpolated)\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.189379Z","iopub.execute_input":"2022-06-20T02:14:11.189874Z","iopub.status.idle":"2022-06-20T02:14:11.375719Z","shell.execute_reply.started":"2022-06-20T02:14:11.189827Z","shell.execute_reply":"2022-06-20T02:14:11.375059Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# In a similar fashion, I can downsample data as well\n# Perhaps I want to predict for quaterly data\n# Perhaps I want to predict for annual data","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.377441Z","iopub.execute_input":"2022-06-20T02:14:11.377709Z","iopub.status.idle":"2022-06-20T02:14:11.381885Z","shell.execute_reply.started":"2022-06-20T02:14:11.377679Z","shell.execute_reply":"2022-06-20T02:14:11.380818Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Power Transforms - Data Transforms ","metadata":{}},{"cell_type":"markdown","source":"**Why do we even need to transform data?**\n1. Data Transformation is required to remove noise(white noise) or improve signal in time series forecasting. \n2. Basically its a procedure to introduce trend into the data","metadata":{}},{"cell_type":"markdown","source":"**Observations**\n1. Identify when to use and how to explore a square root transform \n2. Identify when to use and how to explore a log transform \n3. Using box-cox transform to perform square root and log transforms and identify the best transform available for the dataset ","metadata":{}},{"cell_type":"code","source":"# Airline passengers dataset \nseries = pd.read_csv('../input/air-passengers/AirPassengers.csv', header=0, index_col=0, parse_dates=True)\nseries.columns = series.columns.str.replace('#', '')\nseries = series.squeeze()\nprint(\"\\n\\n\")\n\n# Define the figure \nplt.figure(1)\n\n# Plot Line Plot\nplt.subplot(211)\nseries.plot()\n\n# Print histogram \nplt.subplot(212)\nplt.hist(series)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.383353Z","iopub.execute_input":"2022-06-20T02:14:11.383609Z","iopub.status.idle":"2022-06-20T02:14:11.604677Z","shell.execute_reply.started":"2022-06-20T02:14:11.383576Z","shell.execute_reply":"2022-06-20T02:14:11.604095Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Square Root Transform \n\n# Contrive a time series \nseries = [i**2 for i in range(1, 100)]\nplt.figure(1)\n\n# Construct a line plot \nplt.subplot(211)\nplt.plot(series)\n\n# Construct a histogram \nplt.subplot(212)\nplt.hist(series)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.605914Z","iopub.execute_input":"2022-06-20T02:14:11.606289Z","iopub.status.idle":"2022-06-20T02:14:11.795390Z","shell.execute_reply.started":"2022-06-20T02:14:11.606260Z","shell.execute_reply":"2022-06-20T02:14:11.794573Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"1. Such a series follows a quadratic growth trend. \n2. Its transformation would provide a linear line plot with a uniformly distributed histogram. ","metadata":{}},{"cell_type":"code","source":"# Log Transform AND Exponential Series. Convert them to LINEAR form by taking their LOG\n# As required, add a bias term(intercept) value to convert negative AND zero marked values as their NATURAL LOG isn't defined. ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.799127Z","iopub.execute_input":"2022-06-20T02:14:11.799349Z","iopub.status.idle":"2022-06-20T02:14:11.803146Z","shell.execute_reply.started":"2022-06-20T02:14:11.799322Z","shell.execute_reply":"2022-06-20T02:14:11.802387Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Upsampling calls for interpolations, which can be linear as well as complex\n# Different values of lambda provide different types of transforms\n# lambda = -1.0 is a reciprocal transform.\n# lambda = -0.5 is a reciprocal square root transform.\n# lambda = 0.0 is a log transform.\n# lambda = 0.5 is a square root transform.\n# lambda = 1.0 is no transform","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.804682Z","iopub.execute_input":"2022-06-20T02:14:11.805222Z","iopub.status.idle":"2022-06-20T02:14:11.813079Z","shell.execute_reply.started":"2022-06-20T02:14:11.805181Z","shell.execute_reply":"2022-06-20T02:14:11.812492Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# Smoothing data\n1. Purposes of smoothing - Smoothing data is strongly related to imputing missing data, check for measurement spikes and errors or both \n2. Smoothing is used for Data Preparation, Feature Engineering and even for making predictions. \n3. Smoothing removes noise from time-series and is used to fine-grain values between time-steps.\n\n\n**Moving Average Smoothing**\n1. Specify a window size called the window width. Window width w is slid along the time series to calculate aggregated values. Two main types of moving averages that are used - centered and trailing moving averages. \n\n2. Centered MA - Can be used to remove trend and seasonality componets from the dataset, and not generally used while forecasting. \n\n**Centered and Trailing Moving Average**\n\n**Data Expectations**\n1. Data should be stationary, i.e., there should be no prevalence of trend or seasonality components. They can be removed using the differencing method as described earlier. \n\n**MA as data preparation**\n1. Create a smoothed version of the original dataset. \n2. It reduces the random variation in the latest value with respect to the preceding values and better expose the structure of the underlying causal processes. \n\n\n**Exponential Smoothing**\n1. To be studied later ","metadata":{}},{"cell_type":"code","source":"# The MA-especially the trailing MA, is the same as rolling mean\n# The rolling window works as a lag feature \n\n# Predict female -births using MA \n# Different values of lambda provide different types of transforms\n# lambda = -1.0 is a reciprocal transform.\n# lambda = -0.5 is a reciprocal square root transform.\n# lambda = 0.0 is a log transform.\n# lambda = 0.5 is a square root transform.\n# lambda = 1.0 is no transform\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.815598Z","iopub.execute_input":"2022-06-20T02:14:11.816134Z","iopub.status.idle":"2022-06-20T02:14:11.824623Z","shell.execute_reply.started":"2022-06-20T02:14:11.816091Z","shell.execute_reply":"2022-06-20T02:14:11.823968Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"series = pd.read_csv(\"../input/daily-total-female-births-in-california-1959/daily-total-female-births-CA.csv\", header=0, index_col=0, parse_dates=True,\nsqueeze=True)\n\n# Prepare situation \nX = series.values \nwindow = 3\nhistory = [X[i] for i in range(window)]\ntargets = [X[i] for i in range(window, len(X))]\npreds = []\n\n# Walk forward over time-steps in test data\nfor t in range(len(targets)):\n    k = 0\n    yhat = mean([X[i] for i in range(len(history)-window+k, len(history)+k)])\n    obs = targets[t]\n    preds.append(yhat)\n    history.append(obs)\n    print('predicted=%f, target=%f' % (yhat, obs))\n\n# Calculate RMSE \nrmse = sqrt(mean_squared_error(targets, preds))\nprint(\"The RMSE value is: %.3f\" % rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.826694Z","iopub.execute_input":"2022-06-20T02:14:11.827273Z","iopub.status.idle":"2022-06-20T02:14:11.893608Z","shell.execute_reply.started":"2022-06-20T02:14:11.827228Z","shell.execute_reply":"2022-06-20T02:14:11.892787Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# MA as a prediction ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.894788Z","iopub.execute_input":"2022-06-20T02:14:11.895025Z","iopub.status.idle":"2022-06-20T02:14:11.898711Z","shell.execute_reply.started":"2022-06-20T02:14:11.894995Z","shell.execute_reply":"2022-06-20T02:14:11.897915Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# Temporal Structure in Time Series ","metadata":{}},{"cell_type":"markdown","source":"**Observations**:\n1. White noise is nothing but a random sequence of numbers\n2. Checking if a series is white noise\n3. Statistical and diagnostic plots to check for white noise\n\n**Conditions for a TS to be white noise**\n1. Mean=0 with constant variance\n2. The observations are independent of one another and there is almost negligible or no autocorrelationn. \n3. A WHITE NOISE can't be predicted\n\n**Why it matters?**\n1. **Predictability** - If a time series is white noise, it simply can't be predicted\n2. **Model diagnostics** - The series of errors from a time series forecast model should ideally be a white noise\n\n**How white noise can help**?\n1. If a model blatantly predicts white noise, then its garbage - discard it \n2. If the residuals of a learning algorithm, after prediction on unseen data isn't complete white noise, there is scope of improvement in the model as more and more of main signal's information can be modelled for efficient predictions.\n\n**How to check for white noise**?\n1. Non-zero mean, variability of variance and effect of lagged features - autocorrelation\n2. Create line plots, calculate summary statistics of the entire dataset or its subset and create autocorrelation plots as well","metadata":{}},{"cell_type":"code","source":"# Random Number seed generator \nseed(1)\n\n# Create a white noise series \nseries = [gauss(0.0, 1.0) for i in range(1000)]\nseries = Series(series)\n\n# Print summary statistics \nprint(series.describe())\nprint(\"\\n\\n\")\n\n# Since the sample size is already less, it doesn't make much sense to split the time series as the expected mean and variance of the splits would almost be same \n# Let's plot a line plot \nplt.subplot(211)\nseries.plot()\nplt.show()\n\n# Create a histogram to verify if the distribution is Gaussian \nplt.hist(series)\nplt.show()\n\n# Let's plot an autocorrelation plot for verification \nautocorrelation_plot(series)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:11.899998Z","iopub.execute_input":"2022-06-20T02:14:11.900285Z","iopub.status.idle":"2022-06-20T02:14:12.579896Z","shell.execute_reply.started":"2022-06-20T02:14:11.900246Z","shell.execute_reply":"2022-06-20T02:14:12.579078Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Random Walk in Time Series","metadata":{}},{"cell_type":"code","source":"# Create adn plot a random series \nfrom random import seed\nfrom random import randrange\nseries = [randrange(10) for i in range(1000)]\nplt.plot(series)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:12.580981Z","iopub.execute_input":"2022-06-20T02:14:12.581192Z","iopub.status.idle":"2022-06-20T02:14:12.752647Z","shell.execute_reply.started":"2022-06-20T02:14:12.581166Z","shell.execute_reply":"2022-06-20T02:14:12.752111Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"**This plot is nothing but a sequence of random numbers, not a drunkard's walk**\n\nA random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence. The process used to generate the series forces dependence from one-time step to the next. This dependence provides some consistency from step-to-step rather than the large jumps that a series of independent, random numbers provides. It is this dependency that gives the process its name as a random walk or a drunkard’s walk.","metadata":{}},{"cell_type":"code","source":"# Create and plot a random walk. The seed value is essential for keeping consistency\nfrom random import random\nfrom random import seed\nrandom_walk = []\nseed(1)\nrandom_walk.append(-1 if random() < 0.5 else 1)\nfor i in range(1, 1000):\n    movement = -1 if random() < 0.5 else 1\n    value = random_walk[i-1] + movement \n    random_walk.append(value)\n    \nplt.subplot(311)\nplt.plot(random_walk)\nplt.show()\nprint(\"\\n\\n\")\n\nplt.subplot(312)\nautocorrelation_plot(random_walk)\nplt.show()\nprint(\"\\n\\n\")\nplt.subplot(313)\nplt.hist(random_walk)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:12.753582Z","iopub.execute_input":"2022-06-20T02:14:12.754059Z","iopub.status.idle":"2022-06-20T02:14:13.220694Z","shell.execute_reply.started":"2022-06-20T02:14:12.754027Z","shell.execute_reply":"2022-06-20T02:14:13.219757Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# Running the AD-Fuller test of stationarity \n\n# Import the test package\nfrom statsmodels.tsa.stattools import adfuller \nresult = adfuller(random_walk)\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:13.221780Z","iopub.execute_input":"2022-06-20T02:14:13.221994Z","iopub.status.idle":"2022-06-20T02:14:13.273788Z","shell.execute_reply.started":"2022-06-20T02:14:13.221967Z","shell.execute_reply":"2022-06-20T02:14:13.272816Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Random walk can be made stationary by taking the first difference \ndiff = []\nfor i in range(1, len(random_walk)):\n    value = random_walk[i]-random_walk[i-1]\n    diff.append(value)\nplt.plot(diff)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:14:13.278779Z","iopub.execute_input":"2022-06-20T02:14:13.279835Z","iopub.status.idle":"2022-06-20T02:14:13.479389Z","shell.execute_reply.started":"2022-06-20T02:14:13.279770Z","shell.execute_reply":"2022-06-20T02:14:13.478495Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"**Check if a time series is a random walk**\n1. The time series shows a stromg temporal dependence that decays linearly or in a similar pattern. \n2. The time serie sis non-stationary and making it stationary shows no obviously learnable parameters in the data\n3. The persistence model provides the best source of reliable predictions","metadata":{}},{"cell_type":"code","source":"# Plotting the trend, seasonality and residual of a random walk \nresult_random_walk = seasonal_decompose(random_walk, model='additive', period=1)\nresult_random_walk.plot()\nplt.show()\n\n# There is no residual or seasonality component in a random walk. The given model is just a trend that can be fit. \n# Let's remove the trend\n\nrem_trend = random_walk-result_random_walk.trend\nplt.plot(rem_trend)\nplt.show()\n\n# Hence nothing remains after the trend component has been removed","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:59:13.095433Z","iopub.execute_input":"2022-06-20T03:59:13.095719Z","iopub.status.idle":"2022-06-20T03:59:13.696089Z","shell.execute_reply.started":"2022-06-20T03:59:13.095685Z","shell.execute_reply":"2022-06-20T03:59:13.695221Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# Decomposing a time series data","metadata":{}},{"cell_type":"markdown","source":"# Time Series Components\n**Break a time series down into systematic and unsystematic components**\n1. Systematic: Components of time series that have consistency or recurrence and can be described or modelled. \n2. Non-Systematic: Components of the time series that can't be modelled\n3. It is a useful tool for analysis and helps us witbh forecasting\n4. \n\n\n**A given time series model is thought to consist of three systematic components including level, trend, seasonality and noise. These components are**\n1. Level: Average value in time series\n2. Trend: Increasing or decreasing value in time series\n3. Seasonality: Repeating short-term cycle in time series\n4. Noise: The random variation in the series, i.e, the non-systematic component\n5. Remember, it is not possible to perfecty break a given time series into an additive or multiplicative model ","metadata":{}},{"cell_type":"code","source":"series = [i+randrange(10) for i in range(1, 100)]\nprint(series)\nprint(\"\\n\")\nresult = seasonal_decompose(series, model='additive', period=1)\nprint(\"\\n\")\nresult.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:20:25.892707Z","iopub.execute_input":"2022-06-20T02:20:25.893507Z","iopub.status.idle":"2022-06-20T02:20:26.318906Z","shell.execute_reply.started":"2022-06-20T02:20:25.893465Z","shell.execute_reply":"2022-06-20T02:20:26.317993Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# HENCE, the entire series has been taken as a trend component and there is no seasonaity \n# This form of decomposition is the most naive form of decomposition. There exist some advanced forms like Seasonal and Trend decomposition, using Loess or STL decomposition\n# Caution is advised when using automated decomposition methods","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of Learnings**:\n1. Structure of decomposing time series into level, trend, seasonlaity and noise\n2. Automatically decompose a time series dataset with Python \n3. Decompose an additive or multiplicative model and plot the results ","metadata":{}},{"cell_type":"markdown","source":"# Use and Remove Trends\n1. Importance and types of trends that may exist in a time series and identify them \n2. Use a simple differencing method to remove them \n3. Model a linear trend and remove it from a time series dataset ","metadata":{}},{"cell_type":"markdown","source":"**Types of trends**\n1. Deterministic trends: Those that increase or decrease consistently \n2. Stochastic trends: Those that increase or decrease inconsistently \n\n**Furthermore**\n1. Global and local trends \n\n\n**Identifying a trend**:\n1. Identification and addition or removal of trend is a subjective process. Adding linear and non-linear trends to the data is also subjective\n2. A trend can be prevalent, or added to the forecasting problem as an input \n\n**Methods of Trend Removal**:\n1. De-Trend by differencing \n2. De-Trend by model fitting \n\n\n1. There may exist linear as well as non-linear trends in the dataset\n2. The former can be removed by linear models, whereas the latter can be looked at through non-linear or expoential models","metadata":{}},{"cell_type":"code","source":"# use a linear model to detrend a time series\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\nimport numpy\ndef parser(x):\nreturn datetime.strptime('190'+x, '%Y-%m')\nseries = read_csv('shampoo-sales.csv', header=0, index_col=0, parse_dates=True,\nsqueeze=True, date_parser=parser)\n# fit linear model\nX = [i for i in range(0, len(series))]\nX = numpy.reshape(X, (len(X), 1))\ny = series.values\nmodel = LinearRegression()\nmodel.fit(X, y)\n# calculate trend\ntrend = model.predict(X)\n# plot trend\npyplot.plot(y)\npyplot.plot(trend)\npyplot.show()\n# detrend\ndetrended = [y[i]-trend[i] for i in range(0, len(series))]\n# plot detrended\npyplot.plot(detrended)\npyplot.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using and Removing Seasonality","metadata":{}},{"cell_type":"markdown","source":"1. It may result in a clearer signal\n2. Additional information about the seasonal component of time series can provide new information\n\n**Identification of trend and seasonality components is subjective**:\n1. For example:\n   Time of Day, Daily, Weekly, Monthly and yearly ","metadata":{}},{"cell_type":"markdown","source":"**Seasonal Adjustment with Modelling**:","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}